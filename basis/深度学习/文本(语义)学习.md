## 文本(语义)学习

### 文本分类：

文本分类是自然语言处理中的一个重要任务，目的是将文本分配到一个或多个类别中。简单来说：就是给一段文字打标签。

应用场景：

* **情感分析**：判断评论是积极的、消极的还是中性的。
* **新闻分类**：将新闻文章分到不同的类别，如体育、政治、娱乐等。

:dart:文本的情感分类为例

根据粒度的不同，情感分类可以分为以下几类：（**<u>粒度主要关注分析的精细程度，即我们要分析的对象是整体还是局部，情感的分类是粗略还是精细</u>**）

1. 文档级情感分析：这是粒度最粗的情感分析，分析对象是整个文档或整篇文章。比如，对于一整篇的产品报道或评论，模型会判断这篇文章是正面、负面还是中性
2. 句子级情感分析：句子级情感分析是在文档继承上更加细化，分析每个句子的情感。比如，对于一篇评论中的每一句话，模型都会分别判断它是正面、负面还是中性。
3. 短语级情感分析：更细一步，短语级分析会把注意力放在**句子中的短语或词组**上。比如，“手机的屏幕很好，但电池续航太差了”。这里“屏幕很好”是正面情感，而“电池续航太差”是负面情感。
4. 方面级情感分析：关注具体的**方面**或**属性**。例如，在评价一台手机时，用户可能会对不同方面（如屏幕、续航、性能等）分别表达情感。**特点**：能够明确指出用户对每个具体方面的态度，而不是简单给整个评论打标签。

:oden:细讲文本细粒度分析：<u>捕捉到文本中的细节，比如某个词具体指什么属性（如“颜色”）或表达了什么情感。</u>

目标是：对文本的内容进行更深入、更详细的分析，而不仅仅是整体理解。细粒度分析(fine-grained analysis)意味着把文本拆解成更小的部分，关注更具体的细节。

通俗来讲：在一段产品评论中，我们不仅仅看整体的情感（好评还是差评），还要分析每个部分，像“颜色”、“价格”这些具体的特点是怎么样的。

1. **语义方面**：就是研究每个词（词层面）或者短语的意思，比如“好”是指价格好还是质量好。
2. **属性的抽取与识别**：就是自动找到并标记这些特点，像“颜色”、“大小”等等。

:tickets:基于多模态的情感分析：结合文本、语音、图像等多种信息来识别情感。例如，分析视频中的表情、语气和说话内容，做更全面的情感判断。

### 增量学习与LLM微调

定义：是一种逐步更新模型的方法。模型在接收到新数据后，不需要从头训练，而是能在现有模型基础上“逐步”学习新的信息。这种方法的好处是模型不会遗忘之前学到的东西，同时能持续地适应新数据。

核心特点：

* 持续学习：模型可以在新数据到达时继续学习，而不需要把旧数据重新输入。
* 应对数据变化：适用于数据源不断更新或变化的场景。
* 防止灾难性遗忘：通过特殊机制（如正则化）避免模型学新东西时丢掉旧知识

:scream_cat:区别LLM微调：

定义：指在一个预训练的大型语言模型（Large Language Model，LLM）的基础上，用特定任务的数据进行训练，使模型更适应特定任务或领域。**这个过程通常是在大规模模型的基础上进行的，并且相对较快，因为只需要用小规模的数据对模型进行微调。**

区别：

|             增量学习             | 微调                                                       |
| :------------------------------: | :--------------------------------------------------------- |
| 需要逐步处理数据，不断适应新信息 | 在已有模型基础上，通过**一次性**使用特定任务数据来调整模型 |
|  让模型能持续学习和适应新的数据  | 让模型在特定任务上表现更好。                               |

### 结合LLM微调和增量学习

* 适应特定领域：通过微调，让大语言模型适应某个特定领域或任务，比如法律、医学等
* 持续学习：在部署之后，通过增量学习机制，模型可以不断学习新数据，不仅能适应变化，还能避免遗忘先前知识。

解决两个关键问题：灾难性遗忘和动态数据适应

* 灾难性遗忘：LLM如果不断更新新数据，可能会“忘掉”之前的知识，特别是在不同领域间切换是容易发生这种现象。解决方案可以使用增量学习中的一些技术：
  * 正则化技术：比如Elastic Weight Consolidation，通过对重要的权重施加约束，让模型在学习新任务保留旧任务中重要的参数。
  * 记忆回放：保持一部分旧任务的数据，在学习新数据时加入一小部分旧数据，确保模型不会完全遗忘之前学到的知识。
* 动态数据的适应：为了让模型在部署之后能持续适应新的数据，结合增量学习的思路可以做到：
  * 基于任务的微调：每当新任务或新数据到来时，可以对模型进行小批量的微调（类似在线微调），从而让模型逐渐吸收新知识。
  * 模块化微调：可以设计模块化的微调方式，只微调某些特定的模型层，而保留通用的层。这种方式能够加速微调，并且减少新任务对旧任务的干扰

实际中的操作：

* **初次微调**：首先对LLM进行大规模的初次微调，使其适应特定领域的任务，比如糖尿病管理。这一步确保模型能够应对大部分常见的糖尿病管理中的任务。
* **增量更新**：当新的领域数据出现时，利用增量学习技术对模型进行小批量更新。**这些更新可以是周期性的，比如每周或每月根据新数据做一次小微调。**
* **在线学习与记忆机制**：在模型实际使用的过程中，允许模型“记住”重要的领域知识，并逐步更新。例如，当模型遇到新类型的<u>法律案例或术语</u>时，可以让它从用户交互中学习这些新概念，同时保留之前的基础知识。





















