## LLM Applications

**ollama 是一个基于人工智能的语言模型平台，类似于openAI的GPT系列。它可以处理自然语言任务，如语言生成、问答等。用户可以通过API或应用程序与Ollama模型进行交互，适用于多种应用场景，如聊天机器人、内容生成、自然语言处理等。**

**当你在 Ollama 网站上点击下载时，它会为 macOS 下载一个安装程序。安装后，你可以通过该程序直接在 Mac 上下载和运行大语言模型。只有在运行这个程序前提下才能在terminal中进行下载你要的语言模型**

1. 下载和安装ollama工具
   * 你首先从ollama的官网下载并安装了ollama的命令行工具
   * ollama工具负责<u>从远程服务群拉取模型</u>，并<u>在本地加载</u>和<u>运行</u>它
   * 如果你不使用 `ollama`，需要自己处理模型的加载、推理过程以及硬件优化。（你需要从本地文件系统加载 Llama 3.1 模型文件。通常这些模型文件会非常大，需要高效的加载方法。像 Llama 3.1 这样的模型通常需要依赖特定的深度学习框架（例如 PyTorch 或 TensorFlow）进行推理。你需要使用对应的框架加载模型并进行推理。且硬件加速的配置会变得更加复杂。你需要自己设置 GPU，并确保模型推理过程利用了这些硬件加速器。）
2. 拉取模型
   * 当你运行`ollama pull llama3.1`时，`ollama`会从它的模型仓库中下载Llama 3.1模型。
   * 下载的模型文件会存储在你的本地硬盘上，通常在`ollama`的缓存目录中
   * 这个步骤确保模型已经准备好在本地运行，后续操作无需再从网络下载。
3. 运行模型
   * 当你运行`ollama run llama3.1`时，`ollama`会从本地文件系统中加载已下载好的模型文件。
   * 这意味着模型会被加载到你的本地内存中（RAM），并使用你的CPU或GPU进行推理（生成对话内容）
   * 在macOS上，具体来说，推理过程是直接使用你的设备的硬件资源进行的。如果你的Mac支持GPU加速（如M1或M2芯片），它可能会利用这些硬件来加速推理过程
4. 推理过程
   * 一旦模型被加载，`ollama`工具就开始接收你的输入，进行推理（生成文本响应）。
   * 生成的响应通过终端返回给你，整个过程是在本机上实时进行的，不需要与远程服务器通信。
5. 资源消耗
   * 内存：Llama 3.1模型的大小可能很大（尤其是大参数量的模型），因此在推理时会占用相当多的RAM。你可能会注意到内存使用的增加。
   * **CPU/GPU**：`ollama`会使用你的CPU进行推理。如果你的设备支持GPU推理（如M1/M2芯片的机器），则可以利用GPU加速推理过程。否则，推理全程会依赖CPU，速度可能相对较慢。

### 脚本调用

1. 方法一：

   如果你想通过Python脚本不断地对`llama3.1`模型发起提问并将结果存储下来，可以通过使用`subprocess`调用`ollama`命令来实现。你可以将每次模型的输出保存到一个文件或数据库中。

   如果你一直使用 `subprocess.run` 来调用 `ollama`，每次发起请求都会创建一个新的子进程，这确实会对性能产生不必要的开销。每次都要重新启动模型的加载和推理环境，这会导致大量的资源消耗，特别是在进行频繁、多次交互时。

2. 方法二：

   使用shell脚本

























l
