## 序列神经网络

### 基本介绍

例如你想要建立一个能够自动识别句中人们位置的序列模型

x: "Harry Potter and Hermione Granger invented a new spell."

现在给定这样的输入数据x，希望得到一个序列模型输出y，使得输入的每个单词都对应一个输出值。:jack_o_lantern:这个输出值是能够表明输入的单词是否是属于人名的一部分。

> X     :memo:Harry Potter and Hermione Granger invented a new spell. 序列数据
>             X<1>    X<2>							  		  X<9>
>
> Y           1		1       0 	    1 		1		0	0	0	0
> 	    Y<1>    Y<2>							  		  Y<9>

如何表示一个句子里的单个词汇：one-hot

首先是提出一张词表，你要列出你的表示方法中要用到的单词，如下列词表

![image-20240911205535980](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409112055055.png)

当选定了10000词的词典之后，让这个词典去遍历你的训练集，然后使用one-hot来表示x中的每个单词。

![image-20240911205655675](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409112056702.png)

那么可以使用X<t>来指代句子里的任意词，他就是一个one-hot向量（只有一个值为1，其余值为0）。回到上述的例子，那个句子会有9个one-hot向量来表示句中的9个单词。

:goal_net:这样做的目的：**使序列模型在X和Y之间学习建立一个映射**

### 循环神经网络

先从简单的神经网络看看是否能很好地训练：

![image-20240911211321291](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409112113339.png)

问题：

1. 庞大的输入层
2. 对于不同句子会有不同长度，不是所有的句子都有相同输入长度（输出长度也不一样）
3. 不共享从文本的不同位置上学到的特征

:cyclone:循环神经网络 

> [!IMPORTANT]
>
> 循环神经网络使从左往右扫描数据，同时每个时间步的参数都是共享的。同一个网络在多个不同时间点被使用了多次。

![image-20240911212857876](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409112128902.png)

一般开始先输入a<0>, 它是一个零向量，接着进行前向传播：

![image-20240911213622376](/Users/jiadong/Library/Application Support/typora-user-images/image-20240911213622376.png)

### 词汇表征

 word representation

上面介绍了用one-hot去构建计算机能理解的词表征。这种表示方法的一大缺点就是：**它把每个词都孤立起来，对相关词的泛化能力不强。**

:diamond_shape_with_a_dot_inside:例如你已经学到了一个语言模型，当你看到:  I want a glass of orange ____ .那么这个划线的的词____会是什么，很可能是juice。算法如果学到了那么就会这样填。但如果看到：I want a glass of apple ____ 因为算法不知道 苹果和橙子的关系，所以算法很难从orange juice这一东西去理解 apple juice很常见的

> 根本原因是：任何两个one-hot向量的内积都是0，这些向量之间的距离都一样，所以无法知道apple和orange 要比 man和orange 相似得多。

:key:引入 ： word embedding 词嵌入

我们不再用one-hot去单纯地表示词，而是用特征化的形式去表示每个词：

![image-20240911215112222](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409112151262.png)

那么这样，我们就组成了一个30维的向量来表示'man',‘woman’ ... 这些词。

情况一：现在，我们用这种方式来表示橙子、苹果这些词，那么橙子、苹果的这种表示方法下肯定会非常相似。可能会有特征不太一样，例如颜色、口味... 但总的来说，橙子和苹果的大部分特征实际上都是一样的。**因此，对于知道橙子果汁的算法，大概率很会明白苹果果汁这个东西（橙子果汁是一个训练样本，模型已经见过的了；而苹果果汁是测试样本，是一个完全新的数据）**

情况二：训练集中：Sally Johnson is an orange farmer. 那么在测试集中会出现这样一种例子：Robbert Lin is a durian cultivator. 我们深知对于 **命名实体识别任务**只有一个很小标记的训练集，这个训练集甚至都没有durian(榴莲)，cultivator(培育家)这些词。但是如果你用了词嵌入，<u>它会告诉模型榴莲是水果；培育家跟农民差不多</u>，那么就有可能从你的训练集中的an orange farmer 归纳出榴莲培育家也是一个人，那么Robbert Lin应该也得是个人名（而不能是公司名或者其他）。

#### 使用迁移学习

把从互联网上获取的词嵌入模型（这个模型是在大量的无标签文本中学习到的），然后把它迁移到你的任务中。

1. 自己操作：从大量的文本集中学习词嵌入模型（或者一般选择从网上下载预训练好的词嵌入模型）
2. 用这个词嵌入模型把它迁移到你的新的任务中。把词输入到这个词嵌入中，它就会给你返回一组特征向量

#### 嵌入矩阵

当你应用算法来学习自己的词嵌入矩阵时，实际上是学习一个嵌入矩阵。假设我们的词典有10000个，我们要去学习一个嵌入矩阵E。那么E的shape为(300, 10000)。矩阵的各列代表的是10000个不同的单词

![image-20240911222339125](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409112223172.png)

**使用嵌入矩阵乘以one-hot向量就会得到嵌入向量**。假设字典中的第6738的单词为orange：

![image-20240911223016546](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409112230571.png)

我们的目标是如何学习一个嵌入矩阵E！

#### 如何学习词嵌入：略

### LSTM

RNN

输入：arrive Taipei On November 2nd

**当输入taipei时，hidden layer会同时考虑taipei这个input跟存在memory里面的a1得到a2，然后再根据a2产生y2。**

:eagle:概念

​	LSTM中有三种类型的门：遗忘门、输入门和输出门。遗忘门用于控制上一个时刻的记忆细胞中的信息对当前时刻的记忆细胞的影响。输入门根据当前的输入和上一个时刻的隐藏状态来确定哪些信息应该被更新到记忆细胞中。输出门根据当前的输入和当前时刻的记忆细胞来控制隐藏状态中哪些信息应该被传递给下一个时刻。

* lstm的隐藏层输出包括隐藏状态和记忆细胞。只有隐藏状态会传递到输出层
* lstm的输入门、遗忘门和输出门可以控制信息的流动
* lstm可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系

:cactus:区别一个神经网络隐藏层和lstm网络的隐藏层

**普通神经网络**：

* 普通神经网络（比如前馈神经网络）的隐藏层的神经元非常简单，主要工作就是接受输入，然后通过激活函数产生输出。
* 神经元的计算流程：输入值经过加权求和，接着通过激活函数，再输出到下一层。

**LSTM网络**：

* lstm网络中的隐藏层可以理解为一种特殊的神经元
* 每个lstm单元（本身就是一个特殊的“神经元”）不仅仅是一个简单的神经元，这个神经元中包含了由多个部分组成：**记忆单元（cell state）和门机制**（input gate, forget gate, output gate）。这些部分共同作用来控制信息的存储、更新和输出。
* :satellite:每输入一个时间步的数据，会经过整个lstm层中的多个lstm单元，而不是经过一个lstm单元。<u>这些 LSTM 单元会在同一时间步上**并行处理**输入数据</u>
* :satellite:一个 **LSTM 层** 如果包含 128 个单元，那它就包含 128 个**特殊的神经元**，这些特殊的神经元就是 LSTM 单元。

![image-20240923154340947](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409231543106.png)

:key:记忆细胞和隐藏状态

* 记忆细胞（细胞状态）是LSTM网络的核心组成部分，用于保存和更新信息。它类似于传统循环神经网络中的隐藏状态，但具有额外的能力来控制和过滤信息。

* 隐藏状态是LSTM的输出，它是经过处理的记忆细胞的一种表示。

> [!IMPORTANT]
>
> **细胞状态**更像是一个全局的、长期的记忆系统，它包含了所有时间步的信息（长短期信息皆可）。
>
> **隐藏状态**则是一个即时的、与当前输入直接相关的输出，虽然它通常侧重于短期信息，但也可以根据情况包含长期信息。

### Self-attention

vector set as Input 一堆向量   句子、语音、图(graph)、分子

output：

* 每一个向量都有对应的label。输入输出的长度是一样的。   例如：词汇词性标注

  ![image-20240925180345751](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409251803790.png)

*  只需要输出一个label就好。例如：sentiment analysis  分子graph亲水性如何

  ![image-20240925180603918](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409251806974.png)

* 不知道应该输出多少个labels，但model自己决定。例如：翻译


:racing_car:如何显示： 每一个向量都有对应的label

让FC network考虑更多的信息，比如说上下文的context的信息

![image-20240925181256669](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409251813760.png)

可以给FC network一整个window的信息，让它可以考虑 跟现在要考虑的向量 相邻的其他向量的信息。**问题**：现在有需求不是考虑一个window呢就能够解决的，而是需要考虑一整个sequence才能够解决的，那怎么办？ window大到可以扩起来？不可行，sequence有长有短 

:key:self-attention

运作方式：会吃一整个sequence的信息，你输入几个vectors， 它就输出几个vectors

![image-20240925181925243](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409251819274.png)

> [!CAUTION]
>
> 这四个vectors有什么特别的地方呢，这四个vectors都是考虑了一整个sequences之后得到的。
>
> self-attention: 处理整个sequences的咨讯
>
> FC: 专注于处理某个位置的资讯

:kaaba:如何计算：

self-attention的输入: 有可能是输入X vectors，也有可能是hidden layer的输出（如果是hiddenlayer就表明前面已经对输入做了相关的处理了）

![image-20240925230338610](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409252303733.png)  

这些复杂的箭头：表明：b1考虑了a1-a4产生的   b2, b3, b4都是一样 考虑了整个input的sequence

* 第一个步骤：找出这个sequence里面跟a1相关的其他向量，使用一个特别的机制，利用这个机制，我们可以从很长的sequence里面找出到底哪些部分才是重要的，哪个部分跟判断a1是哪个label是有有关系的，哪个部分是判断a1的回归数值所需要用的资讯。
  * 每一个向量跟a1的关联程度：α  （attention score）
  * 如何自动决定两个向量之间的关联性，给了两个向量a1和a4，它怎么决定a1和a4有多相关，然后给它一个数值α来表示  （Query 可以理解为查询向量。它的作用是在注意力机制中，用来与 Key 进行交互，以确定对不同 Value 的关注程度）
  * ![image-20240925231841696](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409252318905.png)
  * ![image-20240925233625016](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409252336123.png)
* 第二个步骤，使用v（value）它包含了实际的信息内容 。
  * ![image-20240927000058208](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409270001301.png)
  * :yum:例如a1和a2的关联性很强，它们之间的α得到的值很大，那么得到的b1的结果就会很接近 b2的结果（例如a2的那个attention score的分数高，那么v2就会dominate 抽出的结果 那个b1的产出的信息就会很贴近于v2的结果，也就是就是来着a2的信息。（值的大部分是来自a2 因为它们最相关，其余小值来自其他不那么相关的序列信息）

从矩阵计算的角度来解释：

* ![image-20240927001826360](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409270018555.png)

:basketball_man:引入多头注意力机制

> [!NOTE]
>
> 相关性：不同种类的相关性，这时我们需要用不同的q 去找不同的相关性。（more heads）
>
> ![image-20240927002057392](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409270020444.png)

:kaaba:引入位置编码

对于上述的attention中，输入的每个时间步是在sequence的前面，还是在sequence的后面，它是完全不知道的。

![image-20240927002423875](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202409270024950.png)

给每个时间步不同的e，即希望通过给每个位置不同的e，model在处理这个input的时候，它可以知道现在的input它的位置  的资讯是什么样的。（通过某个规则产生的：相对于人设的，还不是model自己学的）

### transformer

本质是一个sequence to sequence     解决输入和输出的长度不相等

![image-20241001154952970](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202410011549130.png)

:waning_crescent_moon:Encoder: 给一排向量，输出一排向量

![image-20241001160942135](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202410011609215.png)

:samoa:Decoder: 解释begin（special token）

![image-20241001162233796](https://cdn.jsdelivr.net/gh/JIaDLu/BlogImg/img/202410011622858.png)

decoder怎么产生一段文本呢？

首先你需要给他一个特殊的符号，这个符号代表开始。这个是一个special token。这个可以用one-hot vector来表示。
