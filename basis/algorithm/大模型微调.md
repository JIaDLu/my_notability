# 大模型改进

### 数据飞轮

1. 数据驱动决策

   首先，通过收集大量的数据，并进行深入分析，企业能够获得对市场、用户和业务的深刻洞察。这些洞察为决策制定提供了依据，使企业能够做出更加明智的决策。

   例如，一家零售企业通过分析销售数据和用户反馈，发现某一产品的销量下降，同时用户对该产品的质量和价格提出了一些意见。基于这些分析结果，企业决定对该产品进行改进，提高质量并调整价格。

2. 决策推动行动

   决策制定后，企业需要将其转化为实际行动。这些行动可以包括产品改进、营销策略调整、运营优化等方面。

   例如，企业根据决策结果，对产品进行了重新设计和生产，并推出了一系列促销活动。同时，企业还优化了供应链管理，提高了物流效率，以确保产品能够及时送达用户手中。

3. 行动产生数据

   行动执行后，会产生新的数据。这些数据可以用来评估行动的效果，并为下一轮的数据分析和决策制定提供依据。

   例如，企业推出新产品和促销活动后，可以通过销售数据、用户反馈等渠道收集新的数据，了解用户对新产品和促销活动的反应。这些数据可以帮助企业评估行动的效果，并为下一步的决策提供参考。

4. 数据反馈循环

   新产生的数据又会进入数据收集环节，开始新一轮的数据分析和决策制定。这样，数据飞轮就不断地循环运转，推动企业持续发展和创新。（**数据反馈：用户与模型的交互会产生新的数据，这些数据可以用来进一步优化模型，通过持续收集用户反馈和交互数据，模型可以不断学习和适应，提高其性能和准确性。**）

   例如，企业通过对新数据的分析，发现新产品和促销活动取得了良好的效果，但也存在一些问题。企业可以根据这些反馈，进一步优化产品和营销策略，不断提高业务绩效。

## 不需要train的LLM改进

### 提示词工程

提示词工程（Prompt Engineering）是一种通过设计和优化输入提示，来引导语言模型逐步推理、分解复杂问题的策略。特别是“思维链提示”（Chain-of-Thought Prompting），能促使模型生成中间步骤，提升其处理多步骤推理任务的准确性和可解释性。然而，提示词工程面临可扩展性和嵌入偏差的挑战，需要精心设计以确保模型在保持效率的同时具备良好的适应性。

### 链式思维：chain of thoughts

:sailboat:定义：在进行推理时，通过一系列中间推理步骤来达到最终结论的过程。对于复杂问题，简单的输入-输出模型往往不足以产生有效的结果，而是需要通过多个推理步骤，逐渐逼近最终的解决方案。

:label:链式思维如何在大型语言模型中自然而然地出现

当模型的规模足够大时，链式思维的能力开始自然涌现。这是因为：

* **学习大量的文本数据**：大型语言模型训练时，接触了大量的文本数据，其中包含许多自然语言中的推理示例。通过训练，这些模型逐渐学会如何将信息连接起来，形成推理链。
* **参数和结构的复杂性**：更大的模型拥有更多的参数和更复杂的结构，这使得模型能够捕捉到更深层次的逻辑关系和信息。

:label:如何理解链式思维提示

在提示中提供一些思维链示范作为范例。具体来说，我们探索语言模型在推理任务中执行少量提示的能力，给出由三元组组成的提示：<输入，思想链，输出>。思维链是导致最终输出的一系列中间自然语言推理步骤，我们将这种方法称为思维链提示。

:american_samoa:优势

1. 对于需要多个推理步骤的问题，链式思维使得模型在计算资源的分配上更加高效。具体来说：

   * **适应性计算**：模型可以根据问题的复杂度，**动态地分配计算资源**。对于需要更多推理步骤的问题，模型可以选择使用更多的计算能力来逐步解决每个子问题。

   * **优化资源利用**：通过识别哪些问题需要更深入的推理，模型能够将计算资源集中在最需要的地方，从而提高整体效率。这种方法在处理复杂推理任务时尤其有效，减少了不必要的计算开销。

2. 只需将思维链序列的示例包含到少样本提示的示例中，就可以在足够大的现成语言模型中轻松引发思维链推理。

   :school_satchel:**少量示例提示**是一种有效的技术，通过给模型提供一小部分示例，帮助它理解如何执行特定的任务。在链式思维的上下文中：

   * **包含链式思维序列的示例**：在进行少量示例提示时，开发者可以将一些具体的链式思维序列作为示例添加到提示中。这些示例展示了如何通过一系列推理步骤得出结论。
   * **模型学习模仿**：通过这些示例，模型能够学习如何在遇到类似问题时，模仿这种逐步推理的方式。模型会试图根据提供的示例来生成自己的推理链。

   :pager:示例说明

   假设我们有一个模型需要解决问题：“一个人在商店里买了5个面包，每个面包的价格是3元。他总共花了多少钱？”我们可以提供如下示例：

   1. **示例1**：
      * 输入：一个人买了5个面包，每个面包3元，他总共花了多少钱？
      * 输出：5个面包的价格是3元 × 5 = 15元。
   2. **示例2**：
      * 输入：如果每个苹果的价格是2元，买3个苹果需要多少钱？
      * 输出：2元 × 3 = 6元。

   在给模型提供这些示例后，<u>模型会学会如何将类似问题分解为中间推理步骤</u>，并最终得出正确的答案。

### 检索增强生成：retrieval-augmented generation(RAG)

定义：large language models with external knowledge bases have been developed. These are used to produce more accurate and contextually relevant text responses. This approach is called **Retrieval-Augmented Generation**

使用：这种迭代方法使用模型的输出作为上下文，根据这个上下文去获取与这个上下文提及的内容有关的信息（知识），那么，随着新的相关知识的引入，模型在进行下一次迭代时，就有了更丰富的信息基础，会产生更好的结果。

> [!important]
>
> 大型语言模型通常需要纳入{现有训练数据集}**未涵盖**的用户特定数据。检索增强生成是一项关键技术，它通过检索外部数据并将其纳入生成过程，有助于提高语言模型回复的准确性和相关性。

### 检索链：Retrieval chain

**Retrieval Chain**是一个更广泛的概念，指的是在语言模型生成输出之前，**检索相关的外部信息**来辅助模型的生成过程。这个过程可以是手动或自动完成的，也可以包含多个步骤。例如，你可以检索多个数据源、对它们进行分析、然后将检索到的上下文提供给模型作为生成依据。

| **方面**             | **Retrieval Chain**                                          | **Retrieval-Augmented Generation (RAG)**                     |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **结构**             | 比较松散，模型和检索机制可以分开处理。模型和检索系统可以分开处理(coze)，可能是串联的。 | 检索和生成紧密结合，它的流程是**检索和生成同时进行**         |
| **检索与生成的关系** | 检索是生成的**辅助工具**，模型用检索到的内容作为上下文来回答。 | 检索是生成的**核心**部分，生成时直接将检索结果融入到生成的输出中。 |
| **检索步骤**         | 可能包括**多轮检索、多个知识库**的结合，甚至与用户互动的过程。 | 通常是一次性完成的检索和生成，不需要多次迭代。               |
| **应用场景**         | 适用于需要多轮查询或复杂任务的场景。                         | 更适用于直接从知识库中**检索和生成复合答案**的任务。         |
| **处理信息方式**     | 检索结果作为上下文辅助模型生成更好的答案。                   | 更加注重在生成过程中直接使用**检索到的文档片段**作为生成内容的一部分，模型不单单是利用上下文，而是可以将检索的结果嵌入到生成的句子中。 |

### AI Agentic Workflow

agentic workflow是一种与LLM交互和完成任务的新方法，它通过将复杂任务分解为多个子任务，并引导LLM逐步完成每个子任务来实现。这种方法不是一次性地向LLM提出一个复杂指令，而是借助AI开发平台将任务分解成多个步骤，在不同环节进行迭代，指导最终生成期望的结果。

Agentic Workflow的实现通常依赖于模块化设计，其中每个模块或组件负责特定的任务或功能。这种设计允许灵活地添加、更新或替换模块，以适应不断变化的业务需求和技术进步。在Agentic Workflow中，AI Agent形成一个网络，每个Agent拥有自己的专长和责任。通过网络协作，它们能够共同处理复杂的任务，实现比单个Agent更高效的工作流程

此外，Agentic Workflow还包括一个自适应循环，其中AI Agent不断收集反馈，评估性能，并根据反馈调整自己的行为，确保了工作流的持续优化和改进。为了与人类用户和其他系统集成，Agentic Workflow提供了交互式界面，支持自然语言交互和可视化操作，提高了用户体验和系统的可访问性

:sailboat:区分AI Agent和Agentic Workflow

* AI Agent侧重于单个智能体的能力和自主性，它们可以独立执行任务或作为更大系统中的一部分。例如，大语言模型可以作为一个AI Agent，它根据你的问题生成合适的回答。
* Agentic Workflow则侧重于多个AI Agent如何组织和协作来完成复杂的任务，它是一种工作流程的设计方法，强调任务分解、迭代优化和模块化。

## 需要train的LLM改进

大型语言模型在许多应用中都需要纳入用户的特定数据，而这些数据并不在模型原有的训练数据集中。为了给用户生成更准确且与上下文相关的回复，需要检索外部数据，并在生成回答时将其纳入LLM。

### 主流的低参数微调

* `LoRA`：

  ```python
  import torch 
  import torch.nn as nn
  import torch.nn.functional as F
  import math
  
  class LoRALinear(nn.Module):
      def __init__(self, in_features, out_features, merge, rank=16, lora_alpha=16, dropout=0.5):
          super(LoRALinear, self).__init__()
          self.in_features = in_features
          self.out_features = out_features
          self.merge = merge
          self.rank = rank
          self.dropout_rate = dropout
          self.lora_alpha = lora_alpha
          
          self.linear = nn.Linear(in_features, out_features)
          if rank > 0:
              self.lora_b = nn.Parameter(torch.zeros(out_features, rank))
              self.lora_a = nn.Parameter(torch.zeros(rank, in_features))
              self.scale = self.lora_alpha / self.rank
              self.linear.weight.requires_grad = False
          
          if self.dropout_rate > 0:
              self.dropout = nn.Dropout(self.dropout_rate)
          else:
              self.dropout = nn.Identity()
          
          self.initial_weights()
      
      def initial_weights(self):
          nn.init.kaiming_uniform_(self.lora_a, a=math.sqrt(5))
          nn.init.zeros_(self.lora_b)
          
      def forward(self, x):
          if self.rank > 0 and self.merge:
              output = F.linear(x, self.linear.weight + self.lora_b @ self.lora_a * self.scale, self.linear.bias)
              output = self.dropout(output)
              return output
          else:
              return self.dropout(self.linear(x))
  
  ```

> [!important]
>
>   lora比较适应模型比较大的情况下。如果你的模型本身比较小/用于微调的数据本身比较小，建议选择用冻结部分参数进行微调。

* Adapter
* Peterning

### 指令数据

| 训练集                                                       | 指令数据                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 这是指模型在预训练过程中使用的所有数据。通常是从互联网、大型文本语料库中收集的，数据内容可能包括各种类型的文章、对话、代码、书籍等。 | **指令数据**则是在模型已有语言能力基础上，让它学习“如何执行任务”和“如何跟随指令”。这是在模型掌握基础语言知识后，通过微调模型（fine-tuning），提升它的具体任务执行能力。 |
| 这些数据可以帮助模型学习语言结构、上下文理解、常识知识等，主要是让模型“理解语言”和“掌握知识”。 | 这些指令数据通常会有明确的输入（比如问题或指令）和期望的输出（比如回答、执行步骤），重点是让模型学会“如何按照指令执行任务”。 |
|                                                              | 指令：翻译成另一种语言；给出这个句子的总结；给定以下段落，总结出主要内容。 |

**用指令数据（instruction data）在预训练模型**的基础上进行进一步调整和优化。

**预训练（Pre-training）**：

* 模型最初通过海量的训练集数据（包括各种文本）进行预训练。这个阶段的目的是让模型学习语言的基本结构、语法、上下文关系以及常识性知识。这个过程通常使用自监督学习，即模型通过预测词汇或句子来进行训练，学习如何理解和生成文本。

**微调（Fine-tuning）**：

* 微调是在预训练模型的基础上，使用更为专门的数据进行训练。在这里，指令数据扮演了关键角色。通过微调，模型不仅仅是理解语言，还需要学会如何“按照指令行事”，即它需要根据给定的任务指令（如回答问题、翻译、总结等）生成符合要求的输出。
* 微调过程就是用指令数据对模型进行“定向优化”，提升它对指令的理解和执行能力。

:radio_button:举个例子：

假设你有一个已经预训练好的模型，它可以回答各种各样的问题。如果你想让它更加准确、有效地回答特定领域的问题（比如医疗领域），你就可以用特定领域的指令数据（如“给定症状，提供诊断建议”）来对模型进行微调。

:black_joker:常见做法：

微调过程中是否**只使用指令数据**还是需要**其他训练数据**，取决于具体的任务和模型的需求。通常情况下，微调过程中主要**使用指令数据**。

* **主要目标**：微调的主要目的是让模型在已有的基础上，学会根据特定任务或指令更好地执行操作。所以，在大部分情况下，微调过程中**只使用指令数据**进行训练就可以了。指令数据包含任务的输入和期望输出，通过这些数据可以让模型专注于学会如何根据指令提供正确的回应。
* **例子**：比如你想让模型学会回答问题、翻译文本或总结段落，只要有足够的指令数据（包括问题-答案、输入-输出对），在微调过程中模型就能逐渐学会如何根据指令产生相应的结果。

:o:引申出 instruction-tuned models: 指在预训练的大模型基础上，使用特定的**指令数据（instruction data）**进行微调得到的模型。

简单来说，预训练的大语言模型（像GPT等）是在大量通用数据上进行训练的，它们学会了广泛的语言知识，但没有特定地学会如何遵循某些任务或指令。而**指令微调（instruction tuning）是进一步使用专门设计的指令数据集**，让模型学会按照人类的要求来处理特定任务。这些数据集通常包含任务指令和相应的期望输出，帮助模型更好地理解和执行类似“回答问题”、“生成代码”等具体任务。

通过这种微调，模型变得更加擅长处理“指令式”任务，用户可以通过给定明确的指令来让模型完成指定的任务，比如你现在的提问。

```python
你是一个大语言模型方向上的专家，请你帮助我理解下面这篇论文的摘要。
```

### RAFT: 在指令微调的过程中加入RAG

:label:user's prompt

> 在使用大模型模型LLM时，用户会给出一个提示，也就是问题或任务说明，这个提示就是所谓的‘prompt’
>
> users' prompt可以理解为：
>
> * 明确任务：帮助模型理解用户的需求，确定要执行的具体任务。例如，如果你给出一个问题作为 prompt，模型就会尝试回答这个问题。
> * 引导输出风格：可以影响模型生成文本的风格、语气、格式等。比如，要求模型以诗歌的形式回答，或者以正式的商务语言回复。
> * 提供背景信息：有时，prompt 中可以包含一些背景信息，让模型在特定的情境下进行回答。这有助于提高回答的准确性和相关性。

`Closed-Book Exam`: LLM在答题过程中无法获得任何其他文件或参考资料的情况

`Open Book Exam`: 将开卷考试比作LLM可以参考外部信息源（外接知识库）的情景。LLM会与一个检索器（retriever）结合使用。这个检索器的作用就是从大量的文档或知识库中检索出与用户问题相关的信息。

`Domain-Specific Open-Book Exam`: 我们预先知道 LLM 将在哪个领域接受测试。具体领域的例子包括企业文件、属于某个组织的代码库等。在所有这些场景中，LLM 都将用于回答问题，而问题的答案可以在文档集合中找到。RAFT<u>更加强调模型的阅读理解能力，而不仅仅是对检索结果的依赖。通过在训练时引入正向文档和干扰文档，模型能在特定领域中表现得更加准确和鲁棒</u>。

| RAG                                                          | RAFT                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 在大多数情况下并不需要额外的训练，因为它主要依赖于现有的检索器来找到相关文档，然后让模型直接从这些文档中生成回答。换句话说，RAG的重点在于通过检索相关文档，模型结合这些检索结果进行推理或生成答案。 | 需要额外的训练，因为它强调的是如何让模型从正向文档和干扰文档中学习，并且提高模型在面对混合信息时的鲁棒性。因此，RAFT不仅仅是依赖检索的结果，而是通过训练，专门适应特定领域，并学会在有干扰信息的情况下更好地做出判断。这就需要模型通过专门的微调（fine-tuning）来提升其能力。 |

### MetaGPT：基于LLM的多智能体系

引入了一种元编程框架，其核心思想是通过将标准化操作程序编码到提示序列中，优化了多个智能体的协作流程。























